{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "train1 = pd.read_csv('train_dataset01.csv')\n",
    "train2 = pd.read_csv('train_dataset02.csv')\n",
    "test = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create new features\n",
    "def create_new_features(data, fluc_window, MA_window):\n",
    "    \"\"\"\n",
    "    Fluctions refer to the sum of differences\n",
    "    between each row and its previous row in a window\n",
    "    \"\"\"\n",
    "    numerics = data.select_dtypes(include=\"float64\")\n",
    "    booleans = data.select_dtypes(include=\"bool\")\n",
    "    \n",
    "    # Calculate fluctuations\n",
    "    flucs = numerics.diff()\n",
    "    flucs = flucs.abs()\n",
    "    flucs = flucs.rolling(fluc_window).sum()\n",
    "    flucs = flucs.fillna(0)\n",
    "    flucs.columns = [str(col) + '_delta_t' for col in flucs.columns]\n",
    "    \n",
    "    # Calculate Moving Average\n",
    "    ma = numerics.rolling(MA_window).mean()\n",
    "    ma = ma.fillna(numerics.mean())\n",
    "    ma.columns = [str(col) + '_MA' for col in ma.columns]\n",
    "    \n",
    "    # Create output\n",
    "    out = pd.concat([data.reset_index(drop=True), flucs], axis=1)\n",
    "    out = pd.concat([out.reset_index(drop=True), ma], axis=1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize features\n",
    "def standardise(data, means, stds):\n",
    "    \"\"\"\n",
    "    Standardise every numeric column based on given\n",
    "    means and stds\n",
    "    \"\"\"\n",
    "    numeric_cols = data.select_dtypes(include=\"float64\").columns\n",
    "    data[numeric_cols] = (data[numeric_cols] - means)/stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build one tree for each feature\n",
    "def create_trees(data):\n",
    "    \"\"\"\n",
    "    For each feature, build a tree to detect anomalies\n",
    "    \"\"\"\n",
    "    trees = {}\n",
    "    for col in data.columns:\n",
    "        if str(col) not in [\"ATT_FLAG\", \"DATETIME\"]:\n",
    "            clf = DecisionTreeClassifier()\n",
    "            clf = clf.fit(data[[col]], data[\"ATT_FLAG\"])\n",
    "            trees[col] = clf\n",
    "\n",
    "    return trees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to vote anomalies from tree results\n",
    "def create_votes(data, trees):\n",
    "    \"\"\"\n",
    "    For each column, use decision tree to decide whether \n",
    "    it is an anomaly. Votes are then summed up into a votes column\n",
    "    \"\"\"\n",
    "    # Predict\n",
    "    votes = pd.DataFrame()\n",
    "    for col in data.columns:\n",
    "        if str(col) in trees:\n",
    "            try:\n",
    "                votes[col] = trees[col].predict(data[[col]])\n",
    "            except Exception:\n",
    "                continue\n",
    "            \n",
    "    # Sum\n",
    "    data[\"votes\"] = votes.sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get scores from votes\n",
    "def create_scores(data, alpha):\n",
    "    \"\"\"\n",
    "    Create score from votes\n",
    "    Each score depends on all previous scores\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in range(len(data[\"votes\"])):\n",
    "        if i > 1:\n",
    "            score = alpha*scores[i-1] + data[\"votes\"][i]\n",
    "        else:\n",
    "            score = data[\"votes\"][i]\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change scores to ATT_FLAG\n",
    "def create_ATT_FLAG(data, scores, min_votes):\n",
    "    \"\"\"\n",
    "    Create dataframe in submission format\n",
    "    \"\"\"\n",
    "    temp = np.asarray(scores)\n",
    "\n",
    "    out = data[[\"DATETIME\"]].copy()\n",
    "    out[\"ATT_FLAG\"] = np.where(temp >= min_votes, 1, 0)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "    \"\"\"\n",
    "    range for floats\n",
    "    \"\"\"\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop continuous columns that do not want to use in trees\n",
    "ignore = [\"LEVEL_T5\", \"FLOW_PU3\", \"FLOW_PU5\", \"FLOW_PU9\"]\n",
    "train2 = train2.drop(ignore, axis=1)\n",
    "\n",
    "# Create new features\n",
    "train2 = create_new_features(train2, 10, 10)\n",
    "test = create_new_features(test, 10, 10)\n",
    "\n",
    "# Learn mean and std from no attack data\n",
    "no_attacks = train2[~train2.ATT_FLAG]\n",
    "means = no_attacks.select_dtypes(include=\"float64\").mean()\n",
    "stds = no_attacks.select_dtypes(include=\"float64\").std()\n",
    "\n",
    "# Standardize\n",
    "standardise(train2, means, stds)\n",
    "standardise(test, means, stds)\n",
    "\n",
    "# Build trees\n",
    "trees = create_trees(train2)\n",
    "\n",
    "# Create votes\n",
    "create_votes(train2, trees)\n",
    "create_votes(test, trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for best decay rate and min_vote\n",
    "bestF1 = 0\n",
    "bestAlpha = 0\n",
    "bestMinVotes = 0\n",
    "y_true = train2.ATT_FLAG\n",
    "\n",
    "for alpha in frange(0.5, 0.99, 0.01):\n",
    "    train2_scores = create_scores(train2, alpha)\n",
    "\n",
    "    for min_votes in range(1, 100):            \n",
    "        train2_out = create_ATT_FLAG(train2, train2_scores, min_votes)\n",
    "\n",
    "        # Calculate F score in train2\n",
    "        y_pred = train2_out.ATT_FLAG == 1\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        if f1 > bestF1:\n",
    "            bestF1 = f1\n",
    "            bestAlpha = alpha\n",
    "            bestMinVotes = min_votes\n",
    "                \n",
    "print(bestF1, bestAlpha, bestMinVotes)\n",
    "# 0.735 0.0150 0.970 94  with fluc (0.015 P was my lower bound)\n",
    "# 0.751 0.0120 0.955 96  with fluc and ma (0.012 P was my lower bound)\n",
    "# 0.780 0.0060 0.970 99  with fluc, ma and above (0.005 P was my lower bound)\n",
    "# 0.781 0.0057 0.969 91  with fluc, ma and above (0.005 P was my lower bound)\n",
    "# 0.764 0.0055 0.972 113 with fluc, ma, above and cat_counts (0.005 P was my lower bound)\n",
    "\n",
    "# 0.772 0.0039 0.976 115 with fluc, ma, above, cat_counts and 20,20 windows\n",
    "# 0.786 0.0034 0.972 87  with fluc, ma, above and 20,20 windows (0.0034 P was my lower bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = create_scores(test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = create_ATT_FLAG(test, test_scores, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('PRESSURE_J300', color=color)\n",
    "ax1.plot(test['LEVEL_T3'], color=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:red'\n",
    "ax2.plot(test_out['ATT_FLAG'], color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out[\"ATT_FLAG\"] = test_out[\"ATT_FLAG\"].astype('bool')\n",
    "test_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_out.to_csv(\"output3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
